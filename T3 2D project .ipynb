{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Task 1: Build a **Multiple Linear Regression model** that predicts the number of deaths in various countries due to COVID-19. You are free to select and choose the dataset you would like to use while building the model and are allowed to use the Pandas Library in your code and can use Excel.\n",
    "\n",
    "The following technical/tool constraint applies: you are NOT allowed to use any existing machine learning packages, such as scikit-learn.\n",
    "\n",
    "As a general guide, you may need to undertake the following actions:\n",
    "- Find data sets for the **number of deaths** in various countries **(a minimum of 20 countries)** due to COVID-19.\n",
    "- Research for appropriate predictor variables to predict deaths due to COVID-19.\n",
    "- You may use time as one of the predictors (in which case you could attempt to predict death rates in the future), or you may choose to leave it out (in which case, you would be looking at the deaths at a fixed chosen point in time).\n",
    "- Find data sets on the chosen predictors for the various countries in the model.\n",
    "- Use **plots** to visualize and understand your data.\n",
    "- Build a **model** and **test the accuracy of your model**, using an appropriately chosen metric(hint: r2 is not a good metric for this task).\n",
    "- If needed, improve your model by incorporating other predictors, and/or removing existing ones.\n",
    "- Discuss your data sets, model, accuracy, and what metrics you used to judge the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data \n",
    "We use data from [Our World in Data](https://github.com/owid/covid-19-data/blob/master/public/data/README.md) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataset by reading the csv file\n",
    "pd.reset_option('display.max_rows')\n",
    "df = pd.read_csv('covid_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(pd.options.display.max_rows)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data\n",
    "Target: `total_deaths`\n",
    "\n",
    "Factors affecting death rates: (according to research)\n",
    "\n",
    "*Factors we have data for and will explore are in **bold***\n",
    "\n",
    "Healthcare Systems (healthcare capacity)\n",
    "- `icu_patients`\n",
    "- `icu_patients_per_million`\n",
    "- `hosp_patients`\n",
    "- `hosp_patients_per_million`\n",
    "- `hospital_beds_per_thousand`\n",
    "- `handwashing_facilities`\n",
    "\n",
    "Nature of disease\n",
    "- `reproduction_rate` (measure no of contacts per unit time)\n",
    "\n",
    "National Policies\n",
    "- `stringency_index` \n",
    "- `total_tests`\n",
    "- `total_vaccinations`\n",
    "\n",
    "Demographics/health\n",
    "- `population_density`\n",
    "- `aged_65_older` (Share of the population that is 65 years and older, most recent year available)\n",
    "- ~~`cardiovasc_death_rate`~~\n",
    "- ~~`diabetes_prevalence`~~\n",
    "\n",
    "Economy\n",
    "- `gdp_per_capita`\n",
    "- `extreme_poverty`\n",
    "\n",
    "Others\n",
    "- `continent`\n",
    "- `location`\n",
    "- `date`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['date','location','continent','population_density','gdp_per_capita','total_deaths',\n",
    "         'extreme_poverty','stringency_index','total_tests','total_vaccinations',\n",
    "         'reproduction_rate','handwashing_facilities','hospital_beds_per_thousand','hosp_patients_per_million',\n",
    "         'hosp_patients','icu_patients_per_million','icu_patients']\n",
    "\n",
    "df_covid = df.loc[((df['location']=='Austria')| (df['location']=='Belgium') | (df['location']=='Bulgaria') | (df['location']=='Croatia') | \n",
    "(df['location']=='Cyprus') | (df['location']=='Czech Republic') | (df['location']=='Denmark') | (df['location']=='Estonia') | (df['location']=='Finland') | \n",
    "(df['location']=='France') | (df['location']=='Germany') | (df['location']=='Greece') | (df['location']=='Hungary') | (df['location']=='Ireland') | \n",
    "(df['location']=='Italy') | (df['location']=='Latvia') | (df['location']=='Lithuania') | (df['location']=='Luxembourg') | \n",
    "(df['location']=='Malta') | (df['location']=='Netherlands') | (df['location']=='Poland') | (df['location']=='Portugal') | (df['location']=='Romania')| \n",
    "(df['location']=='Slovakia')| (df['location']=='Spain')| (df['location']=='Sweden')| (df['location']=='United Kingdom')| (df['location']=='United States')) &\n",
    "((df['date']> '2021-01-01') & (df['date']< '2021-11-17')) ,columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_covid.isna().sum())\n",
    "df_covid=df_covid.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = sns.scatterplot(x='date', y='total_deaths', hue='continent', data=df)\n",
    "myplot.set_title('Total deaths vs date', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_cat=['date','location','continent']\n",
    "columns_num=['population_density','gdp_per_capita','total_deaths',\n",
    "         'extreme_poverty','stringency_index','total_tests','total_vaccinations',\n",
    "         'reproduction_rate','handwashing_facilities','hospital_beds_per_thousand','hosp_patients_per_million',\n",
    "         'hosp_patients','icu_patients_per_million','icu_patients']\n",
    "\n",
    "df_cat = df.loc[((df['location']=='Austria')| (df['location']=='Belgium') | (df['location']=='Bulgaria') | (df['location']=='Croatia') | \n",
    "(df['location']=='Cyprus') | (df['location']=='Czech Republic') | (df['location']=='Denmark') | (df['location']=='Estonia') | (df['location']=='Finland') | \n",
    "(df['location']=='France') | (df['location']=='Germany') | (df['location']=='Greece') | (df['location']=='Hungary') | (df['location']=='Ireland') | \n",
    "(df['location']=='Italy') | (df['location']=='Latvia') | (df['location']=='Lithuania') | (df['location']=='Luxembourg') | \n",
    "(df['location']=='Malta') | (df['location']=='Netherlands') | (df['location']=='Poland') | (df['location']=='Portugal') | (df['location']=='Romania')| \n",
    "(df['location']=='Slovakia')| (df['location']=='Spain')| (df['location']=='Sweden')| (df['location']=='United Kingdom')| (df['location']=='United States')) &\n",
    "((df['date']> '2021-01-01') & (df['date']< '2021-11-17')) ,columns_cat]\n",
    "\n",
    "df_num = df.loc[((df['location']=='Austria')| (df['location']=='Belgium') | (df['location']=='Bulgaria') | (df['location']=='Croatia') | \n",
    "(df['location']=='Cyprus') | (df['location']=='Czech Republic') | (df['location']=='Denmark') | (df['location']=='Estonia') | (df['location']=='Finland') | \n",
    "(df['location']=='France') | (df['location']=='Germany') | (df['location']=='Greece') | (df['location']=='Hungary') | (df['location']=='Ireland') | \n",
    "(df['location']=='Italy') | (df['location']=='Latvia') | (df['location']=='Lithuania') | (df['location']=='Luxembourg') | \n",
    "(df['location']=='Malta') | (df['location']=='Netherlands') | (df['location']=='Poland') | (df['location']=='Portugal') | (df['location']=='Romania')| \n",
    "(df['location']=='Slovakia')| (df['location']=='Spain')| (df['location']=='Sweden')| (df['location']=='United Kingdom')| (df['location']=='United States')) &\n",
    "((df['date']> '2021-01-01') & (df['date']< '2021-11-17')) ,columns_num]\n",
    "\n",
    "def normalize_minmax(dfin):\n",
    "    df_copy=dfin.copy()\n",
    "    min_v=dfin.min(axis=0)\n",
    "    max_v=dfin.max(axis=0)\n",
    "    dfout=(df_copy-min_v)/(max_v-min_v)\n",
    "    return dfout\n",
    "\n",
    "def normalize_z(df):\n",
    "    dfout=(df-df.mean(axis=0))/df.std(axis=0)\n",
    "    return dfout\n",
    "\n",
    "data_norm = normalize_z(df_num)\n",
    "stats = data_norm.describe()\n",
    "# display(stats)\n",
    "\n",
    "frames=[df_cat , data_norm]\n",
    "result = pd.concat(frames,axis=1)\n",
    "result = result.fillna(0)\n",
    "# display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = sns.pairplot(data=result, hue='location',x_vars=['total_deaths','reproduction_rate','handwashing_facilities','hospital_beds_per_thousand','hosp_patients_per_million',\n",
    "         'hosp_patients','icu_patients_per_million','icu_patients'],y_vars=['total_deaths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = sns.pairplot(data=result, hue='location',x_vars=['total_deaths','population_density','gdp_per_capita',\n",
    "         'extreme_poverty','stringency_index','total_tests','total_vaccinations'],y_vars=['total_deaths'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_z(df):\n",
    "    dfout=(df-df.mean(axis=0))/df.std(axis=0)\n",
    "    return dfout\n",
    "\n",
    "def normalize_minmax(dfin):\n",
    "    df_copy=dfin.copy()\n",
    "    min_v=dfin.min(axis=0)\n",
    "    max_v=dfin.max(axis=0)\n",
    "    dfout=(df_copy-min_v)/(max_v-min_v)\n",
    "    return dfout\n",
    "\n",
    "def get_features_targets(df, feature_names, target_names):\n",
    "    df_feature=df.loc[:,feature_names]\n",
    "    df_target=df.loc[:,target_names]\n",
    "    return df_feature, df_target\n",
    "\n",
    "def compute_cost(X, y, beta):\n",
    "    J = 0\n",
    "    #calculate m, no of rows/data pt\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #calculate yp, predicted target value from X and beta\n",
    "    yp = np.matmul(X, beta)\n",
    "    \n",
    "    #calculate the error\n",
    "    error = yp-y\n",
    "    \n",
    "    #calculate the cost\n",
    "    J = (1/(2*m))*np.matmul(error.T, error)\n",
    "    J= J[0][0] #to get the float\n",
    "    return J\n",
    "\n",
    "def prepare_feature(df_feature):\n",
    "    #numpy is just arrays\n",
    "    feature = df_feature.to_numpy()\n",
    "    array1 = np.ones((feature.shape[0],1))\n",
    "    X = np.concatenate((array1, feature), axis = 1)\n",
    "    return X\n",
    "\n",
    "def prepare_target(df_target):\n",
    "    return df_target.to_numpy() \n",
    "\n",
    "def gradient_descent(X, y, beta, alpha, num_iters):\n",
    "    #calculate m from shape of X or y\n",
    "    m = X.shape[0]\n",
    "    J_storage = np.zeros(num_iters)\n",
    "\n",
    "    #for the number of iterations\n",
    "    for n in range(num_iters):\n",
    "        #--> compute the predicted y\n",
    "        yp = np.matmul(X, beta)\n",
    "        \n",
    "        #--> compute the error\n",
    "        error = yp - y\n",
    "        \n",
    "        #--> compute the new beta\n",
    "        beta = beta - (alpha/m)*np.matmul(X.T, error)\n",
    "        \n",
    "        #--> compute J using the new beta and store it\n",
    "        J_storage[n] = compute_cost(X, y, beta)\n",
    "        \n",
    "    return beta, J_storage\n",
    "\n",
    "def predict_norm(X, beta):\n",
    "    y = np.matmul(X, beta)\n",
    "    return y\n",
    "\n",
    "def predict(df_feature, beta):\n",
    "    df_feature = normalize_z(df_feature)\n",
    "    X = prepare_feature(df_feature)\n",
    "    yp = predict_norm(X, beta)\n",
    "    return yp\n",
    "\n",
    "def mean_squared_error(target, pred):\n",
    "    n=target.shape[0]\n",
    "    error=target-pred\n",
    "    mse=(1/n)*np.sum(error**2)\n",
    "    return mse\n",
    "\n",
    "def r2_score(y, ypred):\n",
    "    # calculate ssres\n",
    "    diff = y - ypred\n",
    "    ssres = np.matmul(diff.T, diff)[0][0]\n",
    "    \n",
    "    # calculate sstot\n",
    "    ymean=np.mean(y)\n",
    "    diff_mean=y-ymean #element wise subtraction\n",
    "    sstot= np.matmul(diff_mean.T, diff_mean)[0][0]\n",
    "    \n",
    "    # calcuate r2\n",
    "    return 1-(ssres/sstot)\n",
    "\n",
    "def adj_r2_score(X,y,ypred):\n",
    "    r2=r2_score(y, ypred)\n",
    "    adj_r2=1 - ((1-r2)*(X.shape[0]-1)/(X.shape[0]-X.shape[1]-1))\n",
    "    return adj_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain coefficents via gradient descent and normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_covid.describe())\n",
    "# Get features and targets from data frame\n",
    "feature=['population_density','gdp_per_capita',\n",
    "         'stringency_index','total_tests','total_vaccinations',\n",
    "         'reproduction_rate','hospital_beds_per_thousand','hosp_patients_per_million',\n",
    "         'hosp_patients','icu_patients_per_million','icu_patients']\n",
    "\n",
    "df_feature, df_target = get_features_targets(df_covid, feature, ['total_deaths'])\n",
    "\n",
    "# Normalize features\n",
    "df_feature_z=normalize_z(df_feature)\n",
    "\n",
    "# Export same set of data to excel for excel analysis \n",
    "frames=[df_feature_z , df_target]\n",
    "excel = pd.concat(frames,axis=1)\n",
    "excel = excel.fillna(0)\n",
    "# display(excel)\n",
    "excel.to_excel(\"covid_data_excel.xlsx\")\n",
    "\n",
    "# Prepare X and target vector\n",
    "X = prepare_feature(df_feature_z)\n",
    "m=X.shape[1]\n",
    "target = prepare_target(df_target)\n",
    "\n",
    "# Set up gradient descent\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "beta = np.zeros((m,1))\n",
    "\n",
    "# Call the gradient_descent function\n",
    "beta, J_storage = gradient_descent(X, target, beta, alpha, iterations)\n",
    "# with np.printoptions(threshold=np.inf):\n",
    "#     print(beta)\n",
    "\n",
    "# Normal equation\n",
    "X_sq=np.linalg.inv(np.matmul(X.T,X))\n",
    "Xy=np.matmul(X.T,target)\n",
    "beta_n= np.matmul(X_sq,Xy)\n",
    "\n",
    "#'handwashing_facilities',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(J_storage)\n",
    "plt.title('Cost function')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the predict method to get the y predicted using coeffs from GS and norm eqn \n",
    "pred = predict(df_feature, beta)\n",
    "\n",
    "pred_n = predict(df_feature, beta_n)\n",
    "\n",
    "print(pred)\n",
    "print(pred_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate line fit plots\n",
    "# for i in range(len(feature)):\n",
    "#     ft=feature[i]\n",
    "#     plt.figure(i)\n",
    "#     plt.scatter(df_feature[ft],target)\n",
    "#     plt.scatter(df_feature[ft],pred)\n",
    "#     plt.xlabel(ft)\n",
    "#     plt.ylabel('total number of deaths')\n",
    "#     plt.legend(['actual','predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise residual plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot residuals\n",
    "error=target-pred\n",
    "plt.scatter(pred,error)\n",
    "plt.title('Plot of residuals')\n",
    "plt.xlabel('y value')\n",
    "plt.ylabel('Cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally residual plot should show no discernable pattern. But there is a pattern and this suggests that linear may not be the best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "\n",
    "Reasons for metrics:\n",
    "\n",
    "1. Adjusted R2\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# yes still meaningful, minimum mse with coeff\n",
    "# if model is bad then mse may be big\n",
    "mse=mean_squared_error(target,pred)\n",
    "print('MSE: ',mse)\n",
    "\n",
    "rse=np.sqrt(mse/(X.shape[0]-m-1))\n",
    "print('RSE: ',rse)\n",
    "\n",
    "r2=r2_score(target, pred)\n",
    "print('r2: ',r2)\n",
    "\n",
    "adj_r2=adj_r2_score(X,target,pred)\n",
    "print('adjusted r2: ',adj_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is there a relationship between the response and predictors?\n",
    "\n",
    "We set up a hypothesis test to asnwer this question.\n",
    "\n",
    "Null hypothesis: H0 :β1 =β2 =···=βp =0\n",
    "\n",
    "Alternative hypothesis: Ha : at least one βj is non-zero.\n",
    "\n",
    "This hypothesis test is performed by computing the F-statistic. If Ha is true, then E{(TSS − RSS)/p} > σ2, so we expect F to be greater than 1.\n",
    "\n",
    "F = 14125.72909, Significance F (p value associated with F statistic) = 0 < 0.01\n",
    "The p-value associated with the F-statistic zero, so we have extremely strong evidence that at least one of the factors is associated with increased death rates from covid.\n",
    "\n",
    "For each individual predictor, we also obtained a t-statistic and a p-value. *These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these are exactly equivalent7 to the F- test that omits that single variable from the model, leaving all the others in — i.e. q=1 in (3.24). So it reports the partial effect of adding that variable to the model.*\n",
    "\n",
    "All the p-values of the t statitic is p<0.01. Since the number of predictors is relatively small, this also corroborates the conclusion made using the F-statistic that i the ith variable is related to total deaths of covid in the presence of the other n-1 predictors.\n",
    "\n",
    "2. Which variables are more important?\n",
    "\n",
    "Since we currently have 11 predictor variables, the model may be very complex and be prone to overfitting. Want to decrease model's complexity by selecting only the most important variables. For this we use **backward selection**. Observing the p-values of the various t-statitcs we can see that ```hospital_beds_per_thousand``` seems to have the highest p-value out of all and hence is the least statistically significant. We removed that variable and trained a multi linear regression model on the remaining 10 variables. The resulting model is performs to a similar standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements:\n",
    "- add RSE\n",
    "- add adjusted R2\n",
    "- cleaning data, removing redundancies (quality of data)\n",
    "    - p value for excel\n",
    "    - simulation for ddw\n",
    "- change of model (polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def evalue_models_with_diff_feature_combinations(features):\n",
    "    #1 get all different combinations of features\n",
    "    store_evaluations = []\n",
    "    for L in range(0, len(features)+1):\n",
    "        #2 for each combination, run model, print combination, r2 and adj_r2\n",
    "        for features_subset in itertools.combinations(features, L):\n",
    "            features_list = [*features_subset]\n",
    "            if features_list == []:\n",
    "                continue\n",
    "            #run base model\n",
    "            df_feature, df_target = get_features_targets(df_covid, features_subset, ['total_deaths'])\n",
    "            \n",
    "            # Normalize features\n",
    "            df_feature_z=normalize_z(df_feature)\n",
    "\n",
    "            # Prepare X and target vector\n",
    "            X = prepare_feature(df_feature_z)\n",
    "            m=X.shape[1]\n",
    "            target = prepare_target(df_target)\n",
    "\n",
    "            # Set up gradient descent\n",
    "            iterations = 1500\n",
    "            alpha = 0.01\n",
    "            beta = np.zeros((m,1))\n",
    "\n",
    "            # Call the gradient_descent function\n",
    "            beta, J_storage = gradient_descent(X, target, beta, alpha, iterations)\n",
    "            # with np.printoptions(threshold=np.inf):\n",
    "            #     print(beta)\n",
    "\n",
    "            # Call the predict method to get the y predicted using coeffs from GS and norm eqn \n",
    "            pred = predict(df_feature, beta)\n",
    "\n",
    "            mse_eval=mean_squared_error(target,pred)\n",
    "            \n",
    "            r2_eval = r2_score(target,pred)\n",
    "            \n",
    "            #adj_r2_eval\n",
    "            adj_r2_eval=adj_r2_score(X,target,pred)\n",
    "            \n",
    "            \n",
    "            #print('―' * 10)\n",
    "            #print(f\"feature combination: {features_subset}\")\n",
    "            #print(f\"mse {mse_eval}\")\n",
    "            #print(\"r2\",r2_eval)\n",
    "            #print(\"adj_r2\",adj_r2_eval)\n",
    "            #print('―' * 10)\n",
    "\n",
    "            store_evaluations.append([features_subset,mse_eval,r2_eval,adj_r2_eval])\n",
    "    #print max \n",
    "    #print('―' * 10)\n",
    "    #print('―' * 10)\n",
    "    #print('―' * 10)\n",
    "    \n",
    "    best_fit = max(store_evaluations, key=lambda x: x[3])\n",
    "    print(f\"best_fit combination:\", best_fit[0])\n",
    "    print(f\"best_fit mse:\", best_fit[1])\n",
    "    print(f\"best_fit r2:\", best_fit[2])\n",
    "    print(f\"best_fit adj_r2:\", best_fit[3])\n",
    "    return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evalue_models_with_diff_feature_combinations(feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for collinearity\n",
    "\n",
    "Collinearity refers to the situation in which two or more predictor variables are closely related to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = sns.scatterplot(x='icu_patients', y='icu_patients_per_million', data=df_covid)\n",
    "myplot.set_title('', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = sns.scatterplot(x='hosp_patients_per_million', y='hosp_patients', data=df_covid)\n",
    "myplot.set_title('', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features(df_feature, colname, colname_transformed,power):\n",
    "    df_out=df_feature.copy()\n",
    "    df_out.loc[:,colname_transformed]=df_feature[colname]**power #dont use lambda function, handles stuff differently\n",
    "    # print(df_feature)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feature, df_target = get_features_targets(df_covid, feature, ['total_deaths'])\n",
    "\n",
    "feature=['population_density','gdp_per_capita',\n",
    "         'stringency_index','total_tests','total_vaccinations',\n",
    "         'reproduction_rate','hospital_beds_per_thousand','hosp_patients_per_million',\n",
    "         'hosp_patients','icu_patients_per_million','icu_patients']\n",
    "\n",
    "\n",
    "all={}\n",
    "for i in range(1,5):\n",
    "    df_feature, df_target = get_features_targets(df_covid, feature, ['total_deaths'])\n",
    "    df_feature = transform_features(df_feature, \"hosp_patients\", \"hosp_patients^\"+str(i),i)\n",
    "    df_feature = transform_features(df_feature, \"hosp_patients_per_million\", \"hosp_patients_per_million^\"+str(i),i)\n",
    "    df_feature = transform_features(df_feature, \"icu_patients\", \"icu_patients^\"+str(i),i)\n",
    "    df_feature = transform_features(df_feature, \"icu_patients_per_million\", \"icu_patients_per_million^\"+str(i),i)\n",
    "    \n",
    "    # normalize the feature using z normalization\n",
    "    df_feature_z = normalize_z(df_feature)\n",
    "\n",
    "    # print(df_feature.describe())\n",
    "    X = prepare_feature(df_feature_z)\n",
    "    # print(X)\n",
    "    m=X.shape[1]\n",
    "    target = prepare_target(df_target)\n",
    "\n",
    "    iterations = 1500\n",
    "    alpha = 0.01\n",
    "    beta = np.zeros((m,1))\n",
    "\n",
    "    # call the gradient_descent function\n",
    "    beta, J_storage = gradient_descent(X, target, beta, alpha, iterations)\n",
    "    # print('---',beta)\n",
    "\n",
    "    # call the predict method to get the predicted values\n",
    "    pred = predict(df_feature, beta)\n",
    "    \n",
    "    # print(pred)\n",
    "    all[i]=pred\n",
    "\n",
    "print(all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    pred=all[i]\n",
    "    df_feature, df_target = get_features_targets(df_covid, feature, ['total_deaths'])\n",
    "    df_feature = transform_features(df_feature, \"hosp_patients\", \"hosp_patients^\"+str(i),i)\n",
    "    df_feature = transform_features(df_feature, \"hosp_patients_per_million\", \"hosp_patients_per_million^\"+str(i),i)\n",
    "    df_feature = transform_features(df_feature, \"icu_patients\", \"icu_patients^\"+str(i),i)\n",
    "    df_feature = transform_features(df_feature, \"icu_patients_per_million\", \"icu_patients_per_million^\"+str(i),i)\n",
    "    \n",
    "    plt.figure(i)\n",
    "    plt.scatter(df_feature['hosp_patients^'+str(i)],target)\n",
    "    plt.scatter(df_feature['hosp_patients^'+str(i)],pred)\n",
    "    plt.xlabel('hosp_patients^'+str(i))\n",
    "    plt.ylabel('number of total deaths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_all=[]\n",
    "r2_all=[]\n",
    "adj_r2_all=[]\n",
    "power=list(range(1,5))\n",
    "for i in range(1,5):\n",
    "    print(f\"Power: {i}\")\n",
    "    pred=all[i]\n",
    "    mse2=mean_squared_error(target,pred)\n",
    "    print(\"mse\",mse2)\n",
    "    mse_all.append(mse2)\n",
    "\n",
    "    # why in HW9 can use r2 for mult linreg and polynomial reg\n",
    "    r2_2=r2_score(target, pred)\n",
    "    print(\"r2\",r2_2)\n",
    "    r2_all.append(r2_2)\n",
    "    \n",
    "    adj_r2_2=adj_r2_score(X,target,pred)\n",
    "    print(\"adj_r2\",adj_r2_2)\n",
    "    adj_r2_all.append(adj_r2_2)\n",
    "\n",
    "plt.figure(6)\n",
    "plt.scatter(power,mse_all)\n",
    "plt.figure(7)\n",
    "plt.scatter(power,r2_all)\n",
    "plt.figure(8)\n",
    "plt.scatter(power,adj_r2_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot residuals\n",
    "error=target-pred\n",
    "plt.scatter(pred,error)\n",
    "plt.title('Plot of residuals')\n",
    "plt.xlabel('y value')\n",
    "plt.ylabel('Cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding non-linear terms does not help much to improve model fit to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features_log(df_feature, colname, colname_transformed):\n",
    "    df_out=df_feature.copy()\n",
    "    df_out.loc[:,colname_transformed]=np.log(df_feature[colname]) \n",
    "    # print(df_feature)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feature, df_target = get_features_targets(result, feature, ['total_deaths'])\n",
    "\n",
    "feature=['population_density','gdp_per_capita',\n",
    "         'stringency_index','total_tests','total_vaccinations',\n",
    "         'reproduction_rate','hospital_beds_per_thousand','hosp_patients_per_million',\n",
    "         'hosp_patients','icu_patients_per_million','icu_patients']\n",
    "\n",
    "\n",
    "df_feature, df_target = get_features_targets(df_covid, feature, ['total_deaths'])\n",
    "print(df_feature.describe())\n",
    "df_feature = transform_features_log(df_feature, \"hosp_patients\", \"hosp_patients log\",)\n",
    "df_feature = transform_features_log(df_feature, \"hosp_patients_per_million\", \"hosp_patients_per_million log\")\n",
    "df_feature = transform_features_log(df_feature, \"icu_patients\", \"icu_patients log\")\n",
    "df_feature = transform_features_log(df_feature, \"icu_patients_per_million\", \"icu_patients_per_million log\")\n",
    "    \n",
    "# normalize the feature using z normalization\n",
    "df_feature = normalize_z(df_feature)\n",
    "print(df_feature)\n",
    "X = prepare_feature(df_feature)\n",
    "print(X)\n",
    "m=X.shape[1]\n",
    "target = prepare_target(df_target)\n",
    "\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "beta = np.zeros((m,1))\n",
    "\n",
    "# call the gradient_descent function\n",
    "beta, J_storage = gradient_descent(X, target, beta, alpha, iterations)\n",
    "# print('---',beta)\n",
    "\n",
    "# call the predict method to get the predicted values\n",
    "pred = predict(df_feature, beta)\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(df_feature['hosp_patients log'],target)\n",
    "plt.scatter(df_feature['hosp_patients log'],pred)\n",
    "plt.xlabel('hosp_patients log')\n",
    "plt.ylabel('number of total deaths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- interpret hypo test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D meeting (21/11/21)\n",
    "\n",
    "1. Joe's research\n",
    "Key risk factors:\n",
    "- age (85 and above)\n",
    "- cardio disease\n",
    "- death rates\n",
    "- people vaccinated\n",
    "- pop density\n",
    "- hosp beds\n",
    "- stringency index \n",
    "    - map 2 week's ago stringency index to today's death rate \n",
    "    - based on how long incubation to symptom to death\n",
    "\n",
    "New way of calculating mortality\n",
    "traditional: total death/ infected\n",
    "modified: total death per day/ average number of infectiosn for past few days (icu cases for our case? KIV)\n",
    "\n",
    "Things to do:\n",
    "- HASS component (ST/ Henry)\n",
    "    - Draft 1 (tues)\n",
    "    - Dry run (wed)\n",
    "- Find data for task 2 and confirm the question (Jen)\n",
    "    - No mental health switch to economy\n",
    "    - Use one of the backup question\n",
    "    Question: Predict the economic health in the next financial quarter in post covid times\n",
    "    Target: economic health (employment rate, growth rate)- pick one \n",
    "    Params:\n",
    "    - infected rate/ death rate?\n",
    "    - healthcare spending?\n",
    "    - stringency\n",
    "- SH to come up with prelim model (fix graph) by tmr\n",
    "- filter out countries\n",
    "    - similar gdp\n",
    "    - choose big western europe countries \n",
    "- Joe incorporate comments into model to create v2\n",
    "- Fradt ideas on website (SH)\n",
    "\n",
    "-------------------\n",
    "\n",
    "#### Mon\n",
    "- HASS meeting after test\n",
    "Agenda:\n",
    "- fix the direction of the presentation\n",
    "    - topic\n",
    "    - which writer we are applying\n",
    "    - main points and analysis\n",
    "- leave the making of slides and script to ST and henry\n",
    "\n",
    "#### Tues\n",
    "- Math consult\n",
    "- Meet Samson (3.30-4.30pm)\n",
    "\n",
    "#### Wed\n",
    "- HASS dry run\n",
    "- UPOP (1-6pm, Jen, Joe, SH)\n",
    "\n",
    "#### Thurs\n",
    "- HASS presentation\n",
    "- Polish task 1 and 2\n",
    "- Film video\n",
    "\n",
    "#### Fri\n",
    "**Due time: 6pm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (georgious):\n",
    "\n",
    "1. Why is the plot of pred vs a certain param not a straight line?\n",
    "- the variable could have very little relation to the y (eg Y = 0 X1 + 10 X2)\n",
    "- when you look at the plot of X1 against Y, you are projecting a higher dimension space into a lower dimension space\n",
    "- the values of y could have been affected by many hidden variables that push it away from the expected linear relationship\n",
    "- ie many small pertubations that push the the values of y away from the linear line with x\n",
    "\n",
    "2. How did the model produce that curvy line that seems to model that particular parameter very well?\n",
    "- the effect of many hidden variables?\n",
    "- choosing a very good subset of variables?\n",
    "\n",
    "3. Why is r2 not a good metric?\n",
    "- you can use r2 to compare between diff models with same number of features\n",
    "- not good to use r2 to compare between models with diff num of features because can always arbitarily increase r2 through increasing the number of features\n",
    "- \n",
    "\n",
    "4. What metric should we use to evaluate the model?\n",
    "- adjusted r square?\n",
    "\n",
    "5. How can we use the relationships we observe to pick which features to raise to power? eg observe a pair plot with a log rs between x and y\n",
    "- generally, we will not know the relation between a feature and y just from the pair plot. we use lin reg to figure out the relation between a feature and y\n",
    "- we just try randomly transforming each feature\n",
    "- a small case will if you see a log relationship and then you do a log transformation in the model on that feature\n",
    "\n",
    "6. How do we compare our resulst from GS to normal equation? coeff or error?\n",
    "- both\n",
    "- Why is the coeff not exactly the same?\n",
    "- norm equation assumes X is invertible so there is a unique solution. Is the matrix X invertible? Does the answer have to be unqiue? Can I compare the 2 mse?\n",
    "\n",
    "7. Does the mse here even mean anything? We are deriving the beta from the train set then getting the predicted y from the same train set\n",
    "- yes, we are evaluating the fit of the model to your data, just cannot see how well your model generalises (not the pt of this exercise)\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
