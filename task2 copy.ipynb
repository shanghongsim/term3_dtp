{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Task 2: You are free to find and define a problem (apply the discovery and define phases first, from the UK Design Council Double Diamond, 3.007 Design Thinking and Innovation) of your interest related to COVID-19. The problem can be modelled either using Linear Regression (or Multiple Linear Regression) or Logistic Regression, which means you can work with either continuous numerical data or classification.\n",
    "\n",
    "The following technical/tool constraint applies: you are NOT allowed to use Neural Networks or other Machine Learning models. You must use Python and Jupyter Notebook.\n",
    "\n",
    "In general, you may want to consider performing the following steps:\n",
    "- Find an interesting problem which you want to solve either using **Linear Regression or Classification** (please check with your instructors first on whether the problem makes sense).\n",
    "- Find a **dataset** to build your model. For example, you can use Kaggle (https://www.kaggle.com/datasets) to find suitable datasets.\n",
    "- Use **plots** to visualize and understand your data.\n",
    "- Create **training and test** data sets.\n",
    "- Build your model.\n",
    "- Choose an **appropriate metric** to evaluate your model (you may use the same metric as the one used in Task 1).\n",
    "- Improve your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: predict gold prices given covid\n",
    "Effect of covid on economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data \n",
    "We use data from [Yahoo Finance](https://finance.yahoo.com/quote/GLD/history?period1=1479859200&period2=1637625600&interval=1wk&filter=history&frequency=1wk&includeAdjustedClose=true) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold_all = pd.read_csv('GLD.csv')\n",
    "df = pd.read_csv('covid_data.csv')\n",
    "\n",
    "# display(df_gold_all)\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean covid data by selecting only the relevant columns\n",
    "\n",
    "df_subset = df.loc[((df['location']=='United States')| (df['location']=='China') | (df['location']=='Japan') | (df['location']=='Hong Kong') | (df['location']=='United Kingdom') | \n",
    "(df['location']=='Canada') | (df['location']=='India') | (df['location']=='Saudi Arabia') | (df['location']=='France') | (df['location']=='Germany') | \n",
    "(df['location']=='South Korea') | (df['location']=='Switzerland') | (df['location']=='Australia') | (df['location']=='Netherlands') |  \n",
    "(df['location']=='Iran')| (df['location']=='Sweden')| (df['location']=='Brazil')| (df['location']=='Spain')|(df['location']=='Russia') |(df['location']=='Singapore')) ,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any NaN values\n",
    "df_covid=df_subset.fillna(0)\n",
    "# display(df_covid.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the 'date' column to DateTime object\n",
    "df_covid['Week']=pd.to_datetime(df_covid['date'])\n",
    "\n",
    "# Group by week and then take the mean for the week\n",
    "df_covid=df_covid.groupby(pd.Grouper(key='Week', freq=\"W-MON\")).mean()\n",
    "\n",
    "# Exclude the last week to match up with gold data\n",
    "df_covid=df_covid.iloc[:95,:]\n",
    "# display(df_covid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Up of Gold Data frame\n",
    "# change to standard date \n",
    "\n",
    "df_gold_all = pd.read_csv('GLD.csv')\n",
    "\n",
    "# Change gold date range to same range as covid date range\n",
    "df_gold=df_gold_all.copy()\n",
    "df_gold=df_gold.loc[((df_gold['Date']> '2020-01-26') & (df_gold['Date']< '2021-11-17')) ,:]\n",
    "\n",
    "# Change the date column to DateTime Index\n",
    "df_gold['Date'] = pd.to_datetime(df_gold['Date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync up the indexing for both df_gold and df_covid\n",
    "df_covid=df_covid.reset_index()\n",
    "df_gold=df_gold.reset_index()\n",
    "\n",
    "# Merge into one data frame\n",
    "frames=[df_gold,df_covid]\n",
    "df_all = pd.concat(frames,axis=1)\n",
    "# display(df_all)\n",
    "\n",
    "# Remove duplicated/unecessary columns\n",
    "df_all.drop(['index','Week'], axis=1, inplace=True)\n",
    "# display(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean covid data by selecting only the relevant columns\n",
    "# split data into numerical and categorical set so that we can normalize the numerical set\n",
    "columns_cat=['Date']\n",
    "columns_num=['new_deaths', 'new_cases',\n",
    "         'stringency_index','total_tests','total_vaccinations',\n",
    "         'reproduction_rate','hospital_beds_per_thousand','hosp_patients_per_million',\n",
    "         'hosp_patients','icu_patients_per_million','icu_patients','Open','High','Low','Close','Adj Close','Volume']\n",
    "\n",
    "df_cat = df_all.loc[:,columns_cat]\n",
    "\n",
    "df_num = df_all.loc[:,columns_num]\n",
    "\n",
    "# display(df_num)\n",
    "\n",
    "def normalize_minmax(dfin):\n",
    "    df_copy=dfin.copy()\n",
    "    min_v=dfin.min(axis=0)\n",
    "    max_v=dfin.max(axis=0)\n",
    "    dfout=(df_copy-min_v)/(max_v-min_v)\n",
    "    return dfout\n",
    "\n",
    "def normalize_z(df):\n",
    "    dfout=(df-df.mean(axis=0))/df.std(axis=0)\n",
    "    return dfout\n",
    "\n",
    "df_num_norm = normalize_z(df_num)\n",
    "stats = df_num_norm.describe()\n",
    "# display(stats)\n",
    "\n",
    "frames=[df_cat , df_num_norm]\n",
    "result = pd.concat(frames,axis=1)\n",
    "df_covid=result.fillna(0)\n",
    "# display(df_covid.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean covid data by selecting only the relevant columns\n",
    "columns=['Date','new_deaths', 'new_cases',\n",
    "         'stringency_index','total_tests','total_vaccinations',\n",
    "         'reproduction_rate','hospital_beds_per_thousand','hosp_patients_per_million',\n",
    "         'hosp_patients','icu_patients_per_million','icu_patients','Open','High','Low','Close','Adj Close','Volume']\n",
    "\n",
    "df_cov_gold = df_all.loc[:,columns]\n",
    "df_cov_gold=df_cov_gold.fillna(0)\n",
    "# display(df_cov_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_cov_gold)\n",
    "# df_cov_gold.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean covid data by selecting only the relevant columns\n",
    "# split data into numerical and categorical set so that we can normalize the numerical set\n",
    "columns_cat=['Date']\n",
    "columns_num=['new_deaths', 'new_cases',\n",
    "         'stringency_index','total_tests','total_vaccinations',\n",
    "         'reproduction_rate','hospital_beds_per_thousand','hosp_patients_per_million',\n",
    "         'hosp_patients','icu_patients_per_million','icu_patients','Open','High','Low','Close','Adj Close','Volume']\n",
    "\n",
    "df_cat = df_all.loc[:,columns_cat]\n",
    "\n",
    "df_num = df_all.loc[:,columns_num]\n",
    "\n",
    "# display(df_num)\n",
    "\n",
    "def normalize_minmax(dfin):\n",
    "    df_copy=dfin.copy()\n",
    "    min_v=dfin.min(axis=0)\n",
    "    max_v=dfin.max(axis=0)\n",
    "    dfout=(df_copy-min_v)/(max_v-min_v)\n",
    "    return dfout\n",
    "\n",
    "def normalize_z(df):\n",
    "    dfout=(df-df.mean(axis=0))/df.std(axis=0)\n",
    "    return dfout\n",
    "\n",
    "df_num_norm = normalize_z(df_num)\n",
    "stats = df_num_norm.describe()\n",
    "# display(stats)\n",
    "\n",
    "frames=[df_cat , df_num_norm]\n",
    "df_visual = pd.concat(frames,axis=1)\n",
    "df_visual=df_visual.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = sns.pairplot(data=df_visual, x_vars=['Close','new_deaths', 'new_cases',\n",
    "         'stringency_index','total_tests','total_vaccinations',\n",
    "         'reproduction_rate'],y_vars=['Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = sns.pairplot(data=df_visual, x_vars=['Close','new_deaths','hospital_beds_per_thousand','hosp_patients_per_million',\n",
    "         'hosp_patients','icu_patients_per_million','icu_patients'],y_vars=['Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_minmax(dfin):\n",
    "    df_copy=dfin.copy()\n",
    "    min_v=dfin.min(axis=0)\n",
    "    max_v=dfin.max(axis=0)\n",
    "    dfout=(df_copy-min_v)/(max_v-min_v)\n",
    "    return dfout\n",
    "\n",
    "def normalize_z(df):\n",
    "    dfout=(df-df.mean(axis=0))/df.std(axis=0)\n",
    "    return dfout\n",
    "\n",
    "def get_features_targets(df, feature_names, target_names):\n",
    "    df_feature=df.loc[:,feature_names]\n",
    "    df_target=df.loc[:,target_names]\n",
    "    return df_feature, df_target\n",
    "\n",
    "def compute_cost(X, y, beta):\n",
    "    J = 0\n",
    "    #calculate m, no of rows/data pt\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #calculate yp, predicted target value from X and beta\n",
    "    yp = np.matmul(X, beta)\n",
    "    \n",
    "    #calculate the error\n",
    "    error = yp-y\n",
    "    \n",
    "    #calculate the cost\n",
    "    J = (1/(2*m))*np.matmul(error.T, error)\n",
    "    J= J[0][0] #to get the float\n",
    "    return J\n",
    "\n",
    "def prepare_feature(df_feature):\n",
    "    #numpy is just arrays\n",
    "    feature = df_feature.to_numpy()\n",
    "    array1 = np.ones((feature.shape[0],1))\n",
    "    X = np.concatenate((array1, feature), axis = 1)\n",
    "    return X\n",
    "\n",
    "def prepare_target(df_target):\n",
    "    return df_target.to_numpy() \n",
    "\n",
    "def gradient_descent(X, y, beta, alpha, num_iters):\n",
    "    #calculate m from shape of X or y\n",
    "    m = X.shape[0]\n",
    "    J_storage = np.zeros(num_iters)\n",
    "\n",
    "    #for the number of iterations\n",
    "    for n in range(num_iters):\n",
    "        #--> compute the predicted y\n",
    "        yp = np.matmul(X, beta)\n",
    "        \n",
    "        #--> compute the error\n",
    "        error = yp - y\n",
    "        \n",
    "        #--> compute the new beta\n",
    "        beta = beta - (alpha/m)*np.matmul(X.T, error)\n",
    "        \n",
    "        #--> compute J using the new beta and store it\n",
    "        J_storage[n] = compute_cost(X, y, beta)\n",
    "        \n",
    "    return beta, J_storage\n",
    "\n",
    "def predict_norm(X, beta):\n",
    "    y = np.matmul(X, beta)\n",
    "    return y\n",
    "\n",
    "def predict(df_feature, beta):\n",
    "    df_feature = normalize_z(df_feature)\n",
    "    X = prepare_feature(df_feature)\n",
    "    yp = predict_norm(X, beta)\n",
    "    return yp\n",
    "\n",
    "def mean_squared_error(target, pred):\n",
    "    n=target.shape[0]\n",
    "    error=target-pred\n",
    "    mse=(1/n)*np.sum(error**2)\n",
    "    return mse\n",
    "\n",
    "def split_data(df_feature, df_target, random_state=None, test_size=0.5):\n",
    "    indices=df_target.index\n",
    "\n",
    "    if random_state!=None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    num_rows=len(indices)\n",
    "    k = int(test_size * num_rows)\n",
    "    test_indices=np.random.choice(indices,k,replace=False)\n",
    "\n",
    "    indices=set(indices)\n",
    "    test_indices=set(test_indices)\n",
    "    train_indices=indices-test_indices\n",
    "    \n",
    "    df_feature_train=df_feature.loc[train_indices,:]\n",
    "    df_feature_test=df_feature.loc[test_indices,:]\n",
    "    df_target_train=df_target.loc[train_indices,:]\n",
    "    df_target_test=df_target.loc[test_indices,:]\n",
    "    return df_feature_train, df_feature_test, df_target_train, df_target_test\n",
    "\n",
    "def r2_score(y, ypred):\n",
    "    # calculate ssres\n",
    "    diff = y - ypred\n",
    "    ssres = np.matmul(diff.T, diff)[0][0]\n",
    "    \n",
    "    # calculate sstot\n",
    "    ymean=np.mean(y)\n",
    "    diff_mean=y-ymean #element wise subtraction\n",
    "    sstot= np.matmul(diff_mean.T, diff_mean)[0][0]\n",
    "    \n",
    "    # calcuate r2\n",
    "    return 1-(ssres/sstot)\n",
    "\n",
    "def adj_r2_score(X,y,ypred):\n",
    "    r2=r2_score(y, ypred)\n",
    "    adj_r2=1 - ((1-r2)*(X.shape[0]-1)/(X.shape[0]-X.shape[1]-1))\n",
    "    return adj_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and targets from data frame\n",
    "features=['new_deaths', 'new_cases',\n",
    "         'stringency_index','total_tests','total_vaccinations',\n",
    "         'reproduction_rate','hospital_beds_per_thousand','hosp_patients_per_million',\n",
    "         'hosp_patients','icu_patients_per_million','icu_patients']\n",
    "\n",
    "df_feature, df_target = get_features_targets(df_cov_gold, features, ['Close'])\n",
    "display(df_feature)\n",
    "display(df_target)\n",
    "\n",
    "# Split into training and test data set\n",
    "df_features_train, df_features_test, df_target_train, df_target_test = split_data(df_feature, df_target, random_state=100, test_size=0.3)\n",
    "\n",
    "# Normalize train features\n",
    "df_features_train_z=normalize_z(df_features_train)\n",
    "# display(df_features_train_z.describe())\n",
    "# display(df_target_train.describe())\n",
    "\n",
    "# Export same set of data to excel for excel analysis \n",
    "# frames1=[df_features_train_z, df_target_train]\n",
    "# display(df_features_train_z)\n",
    "# display(df_target_train)\n",
    "# excel1 = pd.concat(frames1,axis=1)\n",
    "# excel1 = excel1.fillna(0)\n",
    "# display(excel1)\n",
    "# excel1.to_excel(\"cov_gold_excel.xlsx\")\n",
    "\n",
    "# Prepare X and target vector\n",
    "X = prepare_feature(df_features_train_z)\n",
    "m=X.shape[1]\n",
    "target = prepare_target(df_target_train)\n",
    "\n",
    "# Set up gradient descent\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "beta = np.zeros((m,1))\n",
    "\n",
    "# Call the gradient_descent function\n",
    "beta, J_storage = gradient_descent(X, target, beta, alpha, iterations)\n",
    "\n",
    "# Normal equation\n",
    "X_sq=np.linalg.inv(np.matmul(X.T,X))\n",
    "Xy=np.matmul(X.T,target)\n",
    "beta_n= np.matmul(X_sq,Xy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(J_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the predict method to get the predicted values\n",
    "pred = predict(df_features_test, beta)\n",
    "with np.printoptions(threshold=np.inf):\n",
    "    print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['new_deaths', 'new_cases',\n",
    "         'stringency_index','total_tests','total_vaccinations',\n",
    "         'reproduction_rate','hospital_beds_per_thousand','hosp_patients_per_million',\n",
    "         'hosp_patients','icu_patients_per_million','icu_patients']\n",
    "target = prepare_target(df_target_test)\n",
    "for i in range(len(features)):\n",
    "    ft=features[i]\n",
    "    plt.figure(i)\n",
    "    plt.scatter(df_features_test[ft],target)\n",
    "    plt.scatter(df_features_test[ft],pred)\n",
    "    plt.xlabel(ft)\n",
    "    plt.ylabel('Close price of gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=mean_squared_error(target,pred)\n",
    "print('MSE: ',mse)\n",
    "\n",
    "rse=np.sqrt(mse/(X.shape[0]-m-1))\n",
    "print('RSE: ',rse)\n",
    "\n",
    "r2=r2_score(target, pred)\n",
    "print('r2: ',r2)\n",
    "\n",
    "adj_r2=adj_r2_score(df_features_test,target,pred)\n",
    "print('adjusted r2: ',adj_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-values of t-Stat in excel\n",
    "<img src=\"./images/excel_task 2_naive.png\">\n",
    "Suggests that\n",
    "- total_tests\n",
    "- total_vaccinations\n",
    "- reproduction_rate\n",
    "- hospital_beds_per_thousand\n",
    "\n",
    "Has no relation to the output variable of gold close price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we confirm with another method that addition of these variables contribute noise and give us a worse model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def evalue_models_with_diff_feature_combinations(features):\n",
    "    #1 get all different combinations of features\n",
    "    store_evaluations = []\n",
    "    for L in range(0, len(features)+1):\n",
    "        #2 for each combination, run model, print combination, r2 and adj_r2\n",
    "        for features_subset in itertools.combinations(features, L):\n",
    "            features_list = [*features_subset]\n",
    "            if features_list == []:\n",
    "                continue\n",
    "            #run base model\n",
    "\n",
    "            # Get features and targets from data frame\n",
    "            df_feature, df_target = get_features_targets(df_cov_gold, features_list, ['Close'])\n",
    "\n",
    "            # Split into training and test data set\n",
    "            df_features_train, df_features_test, df_target_train, df_target_test = split_data(df_feature, df_target, random_state=100, test_size=0.3)\n",
    "\n",
    "            # Normalize train features\n",
    "            df_features_train_z=normalize_z(df_features_train)\n",
    "            \n",
    "            # Prepare X and target vector\n",
    "            X = prepare_feature( df_features_train_z)\n",
    "            m=X.shape[1]\n",
    "            target = prepare_target(df_target_train)\n",
    "\n",
    "            # Set up gradient descent\n",
    "            iterations = 1500\n",
    "            alpha = 0.01\n",
    "            beta = np.zeros((m,1))\n",
    "\n",
    "            # Call the gradient_descent function\n",
    "            beta, J_storage = gradient_descent(X, target, beta, alpha, iterations)\n",
    "\n",
    "            # Call the predict method to get the predicted values\n",
    "            target = prepare_target(df_target_test)\n",
    "            pred = predict(df_features_test, beta)\n",
    "\n",
    "            # Evaluate model\n",
    "            mse=mean_squared_error(target,pred)\n",
    "            #print('MSE: ',mse)\n",
    "\n",
    "            rse=np.sqrt(mse/(X.shape[0]-m-1))\n",
    "            #print('RSE: ',rse)\n",
    "\n",
    "            r2=r2_score(target, pred)\n",
    "            #print('r2: ',r2)\n",
    "\n",
    "            adj_r2=adj_r2_score(df_features_test,target,pred)\n",
    "            #print('adjusted r2: ',adj_r2)\n",
    "            \n",
    "            #print('―' * 10)\n",
    "            #print(f\"feature combination: {features_subset}\")\n",
    "            #print(f\"mse {mse}\")\n",
    "            #print(\"r2_1\",r2)\n",
    "            #print(\"adj_r2_1\",adj_r2)\n",
    "            #print('―' * 10)\n",
    "            \n",
    "            store_evaluations.append([features_subset,mse,r2,adj_r2])\n",
    "    #print max \n",
    "    #print('―' * 10)\n",
    "    #print('―' * 10)\n",
    "    #print('―' * 10)\n",
    "    \n",
    "    best_fit = max(store_evaluations, key=lambda x: x[3])\n",
    "    print(f\"best_fit combination:\", best_fit[0])\n",
    "    print(f\"best_fit mse:\", best_fit[1])\n",
    "    print(f\"best_fit r2:\", best_fit[2])\n",
    "    print(f\"best_fit adj_r2:\", best_fit[3])\n",
    "    return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['new_deaths', 'new_cases',\n",
    "         'stringency_index','total_tests','total_vaccinations',\n",
    "         'reproduction_rate','hospital_beds_per_thousand','hosp_patients_per_million',\n",
    "         'hosp_patients','icu_patients_per_million','icu_patients']\n",
    "\n",
    "# evalue_models_with_diff_feature_combinations(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/best_fit.png\">\n",
    "Simulated results corroborate with what we learn from excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to \n",
    "1. remove variables that have no relation to our target \n",
    "- total_vaccinations\n",
    "- reproduction_rate\n",
    "- hospital_beds_per_thousand\n",
    "\n",
    "2. Add polynomial terms to model\n",
    "- new_cases\n",
    "- hosp_patients_per_million\n",
    "- hosp_patients\n",
    "- icu_patients_per_million\n",
    "- icu_patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features(df_feature, colname, colname_transformed,power):\n",
    "    df_out=df_feature.copy()\n",
    "    for i in range(1,power):\n",
    "        df_out.loc[:,colname_transformed+str(i)]=df_feature[colname]**i \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['new_deaths', 'new_cases', 'stringency_index', 'total_tests', 'icu_patients_per_million', 'icu_patients'] #maybe because of collinearity\n",
    "power_feature=['icu_patients_per_million', 'icu_patients', 'new_cases']\n",
    "# SH version\n",
    "# features=['new_deaths', 'new_cases',\n",
    "#          'stringency_index','total_tests','hosp_patients_per_million',\n",
    "#          'hosp_patients','icu_patients_per_million','icu_patients']\n",
    "# power_feature=['hosp_patients_per_million',\n",
    "#          'hosp_patients','icu_patients_per_million','icu_patients','new_cases']\n",
    "\n",
    "all={}\n",
    "mse_all=[]\n",
    "r2_all=[]\n",
    "adj_r2_all=[]\n",
    "for i in range(1,5):\n",
    "    print(f\"Power: {i}\")\n",
    "    df_feature, df_target = get_features_targets(df_cov_gold, features, ['Close'])\n",
    "    for f in power_feature:\n",
    "        df_feature = transform_features(df_feature, f, f+\"^\",i)\n",
    "\n",
    "    # Split into training and test data set\n",
    "    df_features_train, df_features_test, df_target_train, df_target_test = split_data(df_feature, df_target, random_state=100, test_size=0.3)\n",
    "    \n",
    "    # Normalize train features\n",
    "    df_features_train_z=normalize_z(df_features_train)\n",
    "\n",
    "    # Prepare X and target vector\n",
    "    X = prepare_feature(df_features_train_z)\n",
    "    m=X.shape[1]\n",
    "    target = prepare_target(df_target_train)\n",
    "\n",
    "    # Set up gradient descent\n",
    "    iterations = 1500\n",
    "    alpha = 0.01\n",
    "    beta = np.zeros((m,1))\n",
    "\n",
    "    # Call the gradient_descent function\n",
    "    beta, J_storage = gradient_descent(X, target, beta, alpha, iterations)\n",
    "\n",
    "    # Call the predict method to get the predicted values\n",
    "    pred = predict(df_features_test, beta)\n",
    "    all[i]=pred\n",
    "\n",
    "    # Collect evaluation metric\n",
    "\n",
    "    # MSE\n",
    "    actual=prepare_target(df_target_test)\n",
    "    mse2=mean_squared_error(actual,pred)\n",
    "    print(\"MSE: \",mse2)\n",
    "    mse_all.append(mse2)\n",
    "    \n",
    "    # r_2\n",
    "    r2 = r2_score(actual,pred)\n",
    "    print(\"r2: \",r2)\n",
    "    r2_all.append(r2)\n",
    "\n",
    "    #madj r2\n",
    "    adj_r2=adj_r2_score(df_features_test,actual,pred)\n",
    "    print('adjusted r2: ',adj_r2)\n",
    "    print(\"adj_r2\",adj_r2)\n",
    "    adj_r2_all.append(adj_r2)\n",
    "    print(\"_\" * 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power=list(range(1,5))\n",
    "plt.figure(6)\n",
    "plt.scatter(power,mse_all)\n",
    "plt.xlabel('Power of features')\n",
    "plt.ylabel('Mean squared error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power=list(range(1,5))\n",
    "plt.figure(7)\n",
    "plt.scatter(power,r2_all)\n",
    "plt.xlabel('Power of features')\n",
    "plt.ylabel('r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power=list(range(1,5))\n",
    "plt.figure(8)\n",
    "plt.scatter(power,adj_r2_all)\n",
    "plt.xlabel('Power of features')\n",
    "plt.ylabel('adj r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for collinearity\n",
    "\n",
    "Collinearity refers to the situation in which two or more predictor variables are closely related to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = sns.scatterplot(x='icu_patients', y='icu_patients_per_million', data=df_covid)\n",
    "myplot.set_title('', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myplot = sns.scatterplot(x='hosp_patients_per_million', y='hosp_patients', data=df_covid)\n",
    "myplot.set_title('', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New polynomial model that removes collinear terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``'icu_patients'`` and ``'icu_patients_per_million'`` (similarly for ``'hosp_patients_per_million'`` and ``'hosp_patients'``) are quite collinear. Decided to remove one based on p value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['new_deaths', 'new_cases', 'stringency_index', 'total_tests', 'icu_patients_per_million'] #maybe because of collinearity\n",
    "power_feature=['icu_patients_per_million', 'new_cases']\n",
    "# SH version\n",
    "# features=['new_deaths', 'new_cases',\n",
    "#          'stringency_index','total_tests','hosp_patients_per_million',\n",
    "#          'hosp_patients','icu_patients_per_million','icu_patients']\n",
    "# power_feature=['hosp_patients_per_million',\n",
    "#          'hosp_patients','icu_patients_per_million','icu_patients','new_cases']\n",
    "\n",
    "all={}\n",
    "mse_all=[]\n",
    "r2_all=[]\n",
    "adj_r2_all=[]\n",
    "for i in range(1,10):\n",
    "    print(f\"Power: {i}\")\n",
    "    df_feature, df_target = get_features_targets(df_cov_gold, features, ['Close'])\n",
    "    for f in power_feature:\n",
    "        df_feature = transform_features(df_feature, f, f+\"^\",i)\n",
    "\n",
    "    # Split into training and test data set\n",
    "    df_features_train, df_features_test, df_target_train, df_target_test = split_data(df_feature, df_target, random_state=100, test_size=0.3)\n",
    "    \n",
    "    # Normalize train features\n",
    "    df_features_train_z=normalize_z(df_features_train)\n",
    "\n",
    "    # Prepare X and target vector\n",
    "    X = prepare_feature(df_features_train_z)\n",
    "    m=X.shape[1]\n",
    "    target = prepare_target(df_target_train)\n",
    "\n",
    "    # Set up gradient descent\n",
    "    iterations = 1500\n",
    "    alpha = 0.01\n",
    "    beta = np.zeros((m,1))\n",
    "\n",
    "    # Call the gradient_descent function\n",
    "    beta, J_storage = gradient_descent(X, target, beta, alpha, iterations)\n",
    "\n",
    "    # Call the predict method to get the predicted values\n",
    "    pred = predict(df_features_test, beta)\n",
    "    all[i]=pred\n",
    "\n",
    "    # Collect evaluation metric\n",
    "\n",
    "    # MSE\n",
    "    actual=prepare_target(df_target_test)\n",
    "    mse2=mean_squared_error(actual,pred)\n",
    "    print(\"MSE: \",mse2)\n",
    "    mse_all.append(mse2)\n",
    "    \n",
    "    # r_2\n",
    "    r2 = r2_score(actual,pred)\n",
    "    print(\"r2: \",r2)\n",
    "    r2_all.append(r2)\n",
    "\n",
    "    #madj r2\n",
    "    adj_r2=adj_r2_score(df_features_test,actual,pred)\n",
    "    print('adjusted r2: ',adj_r2)\n",
    "    print(\"adj_r2\",adj_r2)\n",
    "    adj_r2_all.append(adj_r2)\n",
    "    print(\"_\" * 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power=list(range(1,10))\n",
    "plt.figure(6)\n",
    "plt.scatter(power,mse_all)\n",
    "plt.xlabel('Power of features')\n",
    "plt.ylabel('Mean squared error')\n",
    "\n",
    "power=list(range(1,10))\n",
    "plt.figure(7)\n",
    "plt.scatter(power,r2_all)\n",
    "plt.xlabel('Power of features')\n",
    "plt.ylabel('r2')\n",
    "\n",
    "power=list(range(1,10))\n",
    "plt.figure(8)\n",
    "plt.scatter(power,adj_r2_all)\n",
    "plt.xlabel('Power of features')\n",
    "plt.ylabel('adj r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing multicollinearity that is not easily detected using rdige regression (L2 regularisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_minmax(dfin):\n",
    "    df_copy=dfin.copy()\n",
    "    min_v=dfin.min(axis=0)\n",
    "    max_v=dfin.max(axis=0)\n",
    "    dfout=(df_copy-min_v)/(max_v-min_v)\n",
    "    return dfout\n",
    "\n",
    "def normalize_z(df):\n",
    "    dfout=(df-df.mean(axis=0))/df.std(axis=0)\n",
    "    return dfout\n",
    "\n",
    "def get_features_targets(df, feature_names, target_names):\n",
    "    df_feature=df.loc[:,feature_names]\n",
    "    df_target=df.loc[:,target_names]\n",
    "    return df_feature, df_target\n",
    "\n",
    "def compute_cost_ridge(X, y, beta,l):\n",
    "    J = 0\n",
    "    #calculate m, no of rows/data pt\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #calculate yp, predicted target value from X and beta\n",
    "    yp = np.matmul(X, beta)\n",
    "    \n",
    "    #calculate the error\n",
    "    error = yp-y\n",
    "    \n",
    "    #calculate the cost\n",
    "    penalty=l*np.matmul(beta.T,beta)\n",
    "    J = (1/(2*m))*(np.matmul(error.T, error)+penalty)\n",
    "    J= J[0][0] #to get the float\n",
    "    return J\n",
    "\n",
    "def prepare_feature(df_feature):\n",
    "    #numpy is just arrays\n",
    "    feature = df_feature.to_numpy()\n",
    "    array1 = np.ones((feature.shape[0],1))\n",
    "    X = np.concatenate((array1, feature), axis = 1)\n",
    "    return X\n",
    "\n",
    "def prepare_target(df_target):\n",
    "    return df_target.to_numpy() \n",
    "\n",
    "def gradient_descent_ridge(X, y, beta, alpha, num_iters,lam):\n",
    "    #calculate m from shape of X or y\n",
    "    m = X.shape[0]\n",
    "    J_storage = np.zeros(num_iters)\n",
    "\n",
    "    #for the number of iterations\n",
    "    for n in range(num_iters):\n",
    "        #--> compute the predicted y\n",
    "        yp = np.matmul(X, beta)\n",
    "        \n",
    "        #--> compute the error\n",
    "        error = yp - y\n",
    "\n",
    "        #--> compute penalty\n",
    "        penalty=lam*beta\n",
    "        \n",
    "        #--> compute the new beta\n",
    "        delta = np.matmul(X.T, error)+penalty\n",
    "        beta = beta - (alpha/m)*delta\n",
    "        \n",
    "        #--> compute J using the new beta and store it\n",
    "        J_storage[n] = compute_cost_ridge(X, y, beta,lam)\n",
    "        \n",
    "    return beta, J_storage\n",
    "\n",
    "def predict_norm(X, beta):\n",
    "    y = np.matmul(X, beta)\n",
    "    return y\n",
    "\n",
    "def predict(df_feature, beta):\n",
    "    df_feature = normalize_z(df_feature)\n",
    "    X = prepare_feature(df_feature)\n",
    "    yp = predict_norm(X, beta)\n",
    "    return yp\n",
    "\n",
    "def mean_squared_error(target, pred):\n",
    "    n=target.shape[0]\n",
    "    error=target-pred\n",
    "    mse=(1/n)*np.sum(error**2)\n",
    "    return mse\n",
    "\n",
    "def split_data_validation(df_feature, df_target, random_state=None, test_size=0.2, validation_size=0.2):\n",
    "    indices=df_target.index\n",
    "\n",
    "    if random_state!=None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    num_rows=len(indices)\n",
    "    k1 = int(test_size * num_rows)\n",
    "    k2 = int(validation_size * num_rows)\n",
    "    test_indices=np.random.choice(indices,k1,replace=False)\n",
    "    \n",
    "\n",
    "    indices=set(indices)\n",
    "    test_indices=set(test_indices)\n",
    "    remaining_indices=indices-test_indices\n",
    "    validation_indices=np.random.choice(list(remaining_indices),k2,replace=False)\n",
    "    validation_indices=set(validation_indices)\n",
    "    train_indices=indices-test_indices-validation_indices\n",
    "    \n",
    "    df_feature_train=df_feature.loc[train_indices,:]\n",
    "    df_target_train=df_target.loc[train_indices,:]\n",
    "    df_feature_validation=df_feature.loc[validation_indices,:]\n",
    "    df_target_validation=df_target.loc[validation_indices,:]\n",
    "    df_feature_test=df_feature.loc[test_indices,:]\n",
    "    df_target_test=df_target.loc[test_indices,:]\n",
    "    return df_feature_train, df_target_train, df_feature_test, df_target_test,df_feature_validation,df_target_validation\n",
    "\n",
    "def r2_score(y, ypred):\n",
    "    # calculate ssres\n",
    "    diff = y - ypred\n",
    "    ssres = np.matmul(diff.T, diff)[0][0]\n",
    "    \n",
    "    # calculate sstot\n",
    "    ymean=np.mean(y)\n",
    "    diff_mean=y-ymean #element wise subtraction\n",
    "    sstot= np.matmul(diff_mean.T, diff_mean)[0][0]\n",
    "    \n",
    "    # calcuate r2\n",
    "    return 1-(ssres/sstot)\n",
    "\n",
    "def adj_r2_score(X,y,ypred):\n",
    "    r2=r2_score(y, ypred)\n",
    "    # print('r2',r2)\n",
    "    adj_r2=1 - ((1-r2)*(X.shape[0]-1)/(X.shape[0]-X.shape[1]-1))\n",
    "    # print('x:',X.shape[0],X.shape[1])\n",
    "    # print((X.shape[0]-1)/(X.shape[0]-X.shape[1]-1))\n",
    "    # print('adj_r2',adj_r2)\n",
    "    return adj_r2\n",
    "\n",
    "def transform_features(df_feature, colname, colname_transformed,power):\n",
    "    df_out=df_feature.copy()\n",
    "    for i in range(1,power):\n",
    "        df_out.loc[:,colname_transformed+str(i)]=df_feature[colname]**i \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['new_deaths', 'new_cases', 'stringency_index', 'total_tests', 'icu_patients_per_million', 'icu_patients'] #maybe because of collinearity\n",
    "power_feature=['icu_patients_per_million', 'icu_patients', 'new_cases']\n",
    "df_feature, df_target = get_features_targets(df_cov_gold, features, ['Close'])\n",
    "# df_features_train, df_features_test, df_target_train, df_target_test,df_feature_validation,df_target_validation=split_data_validation(df_feature, df_target, random_state=None, test_size=0.2, validation_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRidgeLambda(random_state=100, test_size=0.2, validation_size=0.2):\n",
    "    \n",
    "    \n",
    "    all={}\n",
    "    lam_out=[]\n",
    "    \n",
    "    for j in range(1,10):\n",
    "        lam_all=[]\n",
    "        mse_all=[]\n",
    "        r2_all=[]\n",
    "        adj_r2_all=[]\n",
    "\n",
    "        bestMSE=10e100\n",
    "        best_adjr2=0\n",
    "        best_r2=0\n",
    "\n",
    "        # Raise to appropriate power\n",
    "        print(f\"Power: {j}\")\n",
    "        power_feature=['icu_patients_per_million', 'icu_patients', 'new_cases']\n",
    "        df_feature, df_target = get_features_targets(df_cov_gold, features, ['Close'])\n",
    "        for f in power_feature:\n",
    "            df_feature = transform_features(df_feature, f, f+\"^\",j)\n",
    "        \n",
    "        lamList=[l*0.05 for l in range(0,300)]\n",
    "        \n",
    "        df_features_train, df_target_train, buf1, buf2 ,df_features_validation,df_target_validation=split_data_validation(df_feature, df_target, random_state=100, test_size=0.25, validation_size=0.25)\n",
    "\n",
    "        for i in range(len(lamList)):\n",
    "            l=lamList[i]\n",
    "            \n",
    "            # Normalize train features\n",
    "            df_features_train_z=normalize_z(df_features_train)\n",
    "\n",
    "\n",
    "            # Prepare X and target vector\n",
    "            X = prepare_feature(df_features_train_z)\n",
    "            m=X.shape[1]\n",
    "            target = prepare_target(df_target_train)\n",
    "\n",
    "            # Set up gradient descent\n",
    "            iterations = 1500\n",
    "            alpha = 0.01\n",
    "            beta = np.zeros((m,1))\n",
    "\n",
    "            # Call the gradient_descent function\n",
    "            beta, J_storage=gradient_descent_ridge(X, target, beta, alpha, iterations,l)\n",
    "            \n",
    "            # plt.figure(i)\n",
    "            # plt.plot(J_storage)\n",
    "\n",
    "            actual=prepare_target(df_target_validation)\n",
    "            pred = predict(df_features_validation, beta)\n",
    "            mse=mean_squared_error(actual, pred)\n",
    "            r2=r2_score(actual, pred)\n",
    "            # print(r2)\n",
    "            adj_r2=adj_r2_score(df_features_validation,actual,pred)\n",
    "\n",
    "            if mse< bestMSE and r2>best_r2:\n",
    "                bestMSE=mse\n",
    "                best_r2=r2\n",
    "                best_adjr2=adj_r2\n",
    "                ridgeLambda=l\n",
    "                # print(l)\n",
    "        lam_all.append(ridgeLambda)\n",
    "        lam_out.append(ridgeLambda)\n",
    "        mse_all.append(bestMSE)\n",
    "        r2_all.append(best_r2)\n",
    "        adj_r2_all.append(best_adjr2)\n",
    "        all[j]=[lam_all,mse_all,r2_all,adj_r2_all]\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "    return all,lam_out\n",
    "\n",
    "all,lam_values=getRidgeLambda()\n",
    "print(lam_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['new_deaths', 'new_cases', 'stringency_index', 'total_tests', 'icu_patients_per_million', 'icu_patients'] #maybe because of collinearity\n",
    "power_feature=['icu_patients_per_million', 'icu_patients', 'new_cases']\n",
    "\n",
    "# all={}\n",
    "mse_all_ridge=[]\n",
    "r2_all_ridge=[]\n",
    "adj_r2_all_ridge=[]\n",
    "for i in range(1,10):\n",
    "\n",
    "    # Raise to appropriate power\n",
    "    print(f\"Power: {i}\")\n",
    "    df_feature, df_target = get_features_targets(df_cov_gold, features, ['Close'])\n",
    "    for f in power_feature:\n",
    "        df_feature = transform_features(df_feature, f, f+\"^\",i)\n",
    "\n",
    "    # Split into training and test data set\n",
    "    df_features_train, df_target_train, df_features_test,df_target_test,buf1,buf2=split_data_validation(df_feature, df_target, random_state=100, test_size=0.2, validation_size=0.2)\n",
    "    \n",
    "    # Normalize train features\n",
    "    df_features_train_z=normalize_z(df_features_train)\n",
    "\n",
    "    # Prepare X and target vector\n",
    "    X = prepare_feature(df_features_train_z)\n",
    "    m=X.shape[1]\n",
    "    target = prepare_target(df_target_train)\n",
    "\n",
    "    # Set up gradient descent\n",
    "    iterations = 1500\n",
    "    alpha = 0.01\n",
    "    beta = np.zeros((m,1))\n",
    "    lam=1\n",
    "    #lam_values[i-1]\n",
    "    print(lam)\n",
    "\n",
    "    # Call the gradient_descent function\n",
    "    beta, J_storage = gradient_descent_ridge(X, target, beta, alpha, iterations,lam)\n",
    "\n",
    "    # Call the predict method to get the predicted values\n",
    "    pred = predict(df_features_test, beta)\n",
    "    all[i]=pred\n",
    "\n",
    "    # Collect evaluation metric\n",
    "    # MSE\n",
    "    actual=prepare_target(df_target_test)\n",
    "    mse2=mean_squared_error(actual,pred)\n",
    "    print(\"MSE: \",mse2)\n",
    "    mse_all_ridge.append(mse2)\n",
    "    \n",
    "    # r_2\n",
    "    r2 = r2_score(actual,pred)\n",
    "    print(\"r2: \",r2)\n",
    "    r2_all_ridge.append(r2)\n",
    "\n",
    "    #madj r2\n",
    "    adj_r2=adj_r2_score(df_features_test,actual,pred)\n",
    "    print('adjusted r2: ',adj_r2)\n",
    "    adj_r2_all_ridge.append(adj_r2)\n",
    "    print(\"_\" * 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power=list(range(1,10))\n",
    "plt.figure(6)\n",
    "plt.scatter(power,mse_all)\n",
    "plt.scatter(power,mse_all_ridge)\n",
    "plt.xlabel('Power of features')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.legend(['normal', 'with ridge regression'])\n",
    "\n",
    "power=list(range(1,10))\n",
    "plt.figure(7)\n",
    "plt.scatter(power,r2_all)\n",
    "plt.scatter(power,r2_all_ridge)\n",
    "plt.xlabel('Power of features')\n",
    "plt.ylabel('r2')\n",
    "plt.legend(['normal', 'with ridge regression'])\n",
    "\n",
    "power=list(range(1,10))\n",
    "plt.figure(8)\n",
    "plt.scatter(power,adj_r2_all)\n",
    "plt.scatter(power,adj_r2_all_ridge)\n",
    "plt.xlabel('Power of features')\n",
    "plt.ylabel('adj r2')\n",
    "plt.legend(['normal', 'with ridge regression'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 regularisation seems to make model much worse by increasing mse and decreasing adjusted r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab98593b63df34d96b9cf9028151dd1be88d2ccc5f4e0f03580fbedb877f8576"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
